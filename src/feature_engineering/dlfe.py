"""
åŠ¨æ€å±€éƒ¨ç‰¹å¾åµŒå…¥æ¨¡å— (DLFE)
åŠŸèƒ½ï¼šåŸºäºADMMç®—æ³•çš„æµå½¢å­¦ä¹ ï¼Œå®ç°åŠ¨æ€ç‰¹å¾é™ç»´
ä½œè€…ï¼šDLFE-LSTM-WSI Team
æ—¥æœŸï¼š2025-09-26
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Union
from pathlib import Path
import logging
import json
import pickle
import sys
import time
from scipy.sparse.linalg import eigsh
from scipy.linalg import eigh
import warnings

try:
    import torch
    TORCH_AVAILABLE = True
    if torch.cuda.is_available():
        DEFAULT_DEVICE = "cuda"
    else:
        DEFAULT_DEVICE = "cpu"
except ImportError:
    torch = None
    TORCH_AVAILABLE = False
    DEFAULT_DEVICE = "cpu"

warnings.filterwarnings('ignore', category=RuntimeWarning)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def diagnose_matrix_sparsity(matrix: np.ndarray, name: str = "Matrix") -> Dict:
    """
    Diagnose matrix sparsity and estimate memory impact.

    Args:
        matrix: Matrix to inspect.
        name: Matrix name for logging.

    Returns:
        Dictionary containing diagnostic metrics.
    """
    n, m = matrix.shape
    total_elements = n * m

    nonzero_mask = np.abs(matrix) > 1e-10
    nonzero_elements = np.count_nonzero(nonzero_mask)

    sparsity = 1.0 - (nonzero_elements / total_elements)

    dense_memory_gb = (total_elements * 8) / (1024 ** 3)
    sparse_memory_mb = (nonzero_elements * 12) / (1024 ** 2)

    savings_ratio = (dense_memory_gb * 1024) / sparse_memory_mb if sparse_memory_mb > 0 else float("inf")

    nonzero_per_row = np.sum(nonzero_mask, axis=1)
    avg_nonzero_per_row = float(np.mean(nonzero_per_row))
    max_nonzero_per_row = int(np.max(nonzero_per_row))
    min_nonzero_per_row = int(np.min(nonzero_per_row))

    result = {
        "shape": (n, m),
        "total_elements": int(total_elements),
        "nonzero_elements": int(nonzero_elements),
        "sparsity": float(sparsity),
        "avg_nonzero_per_row": avg_nonzero_per_row,
        "max_nonzero_per_row": max_nonzero_per_row,
        "min_nonzero_per_row": min_nonzero_per_row,
        "dense_memory_gb": float(dense_memory_gb),
        "sparse_memory_mb": float(sparse_memory_mb),
        "savings_ratio": float(savings_ratio),
    }

    diag_logger = logging.getLogger(__name__)
    diag_logger.info("=" * 70)
    diag_logger.info(f"ğŸ“Š {name} ç¨€ç–æ€§è¯Šæ–­æŠ¥å‘Š")
    diag_logger.info("=" * 70)
    diag_logger.info(f"çŸ©é˜µå½¢çŠ¶: {n} Ã— {m}")
    diag_logger.info(f"æ€»å…ƒç´ æ•°: {total_elements:,}")
    diag_logger.info(f"éé›¶å…ƒç´ : {nonzero_elements:,}")
    diag_logger.info(f"ç¨€ç–åº¦: {sparsity * 100:.2f}%")
    diag_logger.info(f"æ¯è¡Œéé›¶å…ƒç´ : å¹³å‡={avg_nonzero_per_row:.1f}, æœ€å¤§={max_nonzero_per_row}, æœ€å°={min_nonzero_per_row}")
    diag_logger.info("")
    diag_logger.info("ğŸ’¾ å†…å­˜å ç”¨ä¼°ç®—:")
    diag_logger.info(f"  å¯†é›†å­˜å‚¨ (float64): {dense_memory_gb:.2f} GB")
    diag_logger.info(f"  ç¨€ç–å­˜å‚¨ (COO):     {sparse_memory_mb:.2f} MB")
    diag_logger.info(f"  èŠ‚çœæ¯”ä¾‹:           {savings_ratio:.1f}x")
    diag_logger.info("")

    should_use_sparse = sparsity > 0.9 and dense_memory_gb > 1.0
    if should_use_sparse:
        diag_logger.info("âœ… æ¨èä½¿ç”¨ç¨€ç–çŸ©é˜µä¼˜åŒ–ï¼")
        diag_logger.info(f"   ç†ç”±: ç¨€ç–åº¦={sparsity * 100:.1f}%, å¯†é›†å­˜å‚¨éœ€è¦={dense_memory_gb:.2f}GB")
    else:
        diag_logger.info("âš ï¸  ç¨€ç–çŸ©é˜µä¼˜åŒ–æ”¶ç›Šä¸æ˜æ˜¾")
        diag_logger.info(f"   ç†ç”±: ç¨€ç–åº¦={sparsity * 100:.1f}%, å¯†é›†å­˜å‚¨ä»…éœ€={dense_memory_gb:.2f}GB")

    diag_logger.info("=" * 70)

    result["should_use_sparse"] = should_use_sparse
    return result


class DLFE:
    """
    Dynamic Local Feature Embedding (DLFE)

    Manifold-learning dimensionality reduction solved via ADMM while
    preserving local neighbourhood structure in DPSR outputs.

    Attributes:
        target_dim (int): target embedding dimension (default 30)
        sigma (float): RBF kernel parameter (fixed 1.0 per Gu et al. 2025)
        alpha (float): ADMM balance parameter (2^-10 approx 0.00098)
        beta (float): ADMM regularisation parameter (0.1, sparsity)
        max_iter (int): ADMM maximum iterations
        tol (float): convergence tolerance
    """

    def __init__(self,
                 target_dim: int = 30,
                 sigma: float = 1.0,
                 alpha: float = 0.0009765625,
                 beta: float = 0.1,
                 max_iter: int = 100,
                 tol: float = 1e-5,
                 device: str = "auto",
                 use_float32_eigh: bool = False,
                 use_sparse_matrix: bool = True):
        """
        åˆå§‹åŒ–DLFE

        Args:
            target_dim: target embedding dimension (30).
            sigma: RBF kernel width (fixed 1.0 per Gu et al. 2025).
            alpha: ADMM balance parameter (2^-10 approx 0.00098).
            beta: ADMM sparsity regularisation parameter (0.1).
            max_iter: ADMM maximum iterations.
            tol: convergence tolerance.
            device: compute device ('auto', 'cuda', 'cpu').
            use_float32_eigh: æ˜¯å¦å¯ç”¨ GPU ç‰¹å¾åˆ†è§£ï¼ˆlobpcgï¼‰ï¼Œå¤§å‹ç¨€ç–çŸ©é˜µæ—¶å¯èƒ½è§¦å‘ OOMã€‚
            use_sparse_matrix: æ˜¯å¦å¯ç”¨ç¨€ç–çŸ©é˜µä¼˜åŒ–ï¼ˆCPU eigshï¼‰ï¼Œç¨€ç–åº¦>95%æ—¶æ¨èã€‚
        """
        self.target_dim = target_dim
        self.sigma = sigma
        self.alpha = alpha
        self.beta = beta
        self.max_iter = max_iter
        self.tol = tol
        self.use_float32_eigh = use_float32_eigh
        self.use_sparse_matrix = use_sparse_matrix

        # Device management (GPU acceleration path)
        self.use_gpu = False
        self.device = "cpu"
        self._torch = torch if TORCH_AVAILABLE else None
        self._torch_device = None

        if TORCH_AVAILABLE:
            selected_device = device
            if selected_device not in ("auto", "cpu", "cuda"):
                logger.warning("æœªçŸ¥è®¾å¤‡ç±»å‹%sï¼Œå·²å›é€€åˆ°è‡ªåŠ¨æ£€æµ‹æ¨¡å¼", selected_device)
                selected_device = "auto"

            if selected_device == "auto":
                selected_device = DEFAULT_DEVICE

            if selected_device == "cuda" and not torch.cuda.is_available():
                logger.warning("CUDAä¸å¯ç”¨ï¼ŒDLFEå°†ä½¿ç”¨CPUæ¨¡å¼æ‰§è¡Œè®¡ç®—")
                selected_device = "cpu"

            self.device = selected_device
            self.use_gpu = selected_device == "cuda"
            self._torch_device = torch.device(selected_device)

            if self.use_gpu:
                try:
                    device_name = torch.cuda.get_device_name(self._torch_device)
                except Exception:  # pragma: no cover - é©±åŠ¨å®ç°å·®å¼‚
                    device_name = "CUDA"
                logger.info("DLFEå¯ç”¨GPUåŠ é€Ÿï¼š%s", device_name)
            else:
                logger.info("DLFEè¿è¡Œåœ¨CPUæ¨¡å¼ (PyTorchæ£€æµ‹åˆ°ï¼Œä½†æœªå¯ç”¨CUDA)")
        else:
            if device == "cuda":
                logger.warning("æœªå®‰è£…PyTorchï¼Œæ— æ³•å¯ç”¨CUDAï¼›DLFEå°†ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("DLFEè¿è¡Œåœ¨CPUæ¨¡å¼ (PyTorchä¸å¯ç”¨)")

        # æ˜ å°„çŸ©é˜µ
        self.mapping_matrix = None  # AçŸ©é˜µ
        self.is_fitted = False

        # ä¼˜åŒ–å†å²
        self.optimization_history = {
            'objective': [],
            'constraint_violation': [],
            'relative_change': [],
            'iterations': 0
        }
        self._sparsity_diagnosis: Optional[Dict] = None

        logger.info(f"DLFEåˆå§‹åŒ–: ç›®æ ‡ç»´åº¦={target_dim}, Ïƒ={sigma}, Î±={alpha}, Î²={beta}")

    def build_similarity_matrix(self,
                              X: np.ndarray,
                              weights: Optional[np.ndarray] = None,
                              k_neighbors: Optional[int] = None) -> np.ndarray:
        """
        æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µQ

        ä½¿ç”¨é«˜æ–¯æ ¸è®¡ç®—æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦ï¼š
        Qij = exp(-dw(si-sj)Â²/ÏƒÂ²) if sj âˆˆ Si(si)
              0                    otherwise

        Args:
            X: è¾“å…¥æ•°æ® (n_samples x n_features)
            weights: DPSRæä¾›çš„ç‰¹å¾æƒé‡ï¼ˆå¯é€‰ï¼‰
            k_neighbors: kè¿‘é‚»æ•°é‡ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨å…¨è¿æ¥ï¼‰

        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µQ (n_samples x n_samples)
        """
        if self.use_gpu and self._torch is not None:
            return self._build_similarity_matrix_gpu(X, weights, k_neighbors)

        n_samples, n_features = X.shape

        # å¦‚æœæä¾›äº†æƒé‡ï¼Œè¿›è¡ŒåŠ æƒ
        if weights is not None:
            if len(weights.shape) == 1:
                # ä¸€ç»´æƒé‡ï¼Œå¹¿æ’­åˆ°æ‰€æœ‰ç‰¹å¾
                X_weighted = X * np.sqrt(weights.reshape(1, -1))
            else:
                # å¤šç»´æƒé‡ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸åŒæƒé‡ï¼‰
                X_weighted = X * np.sqrt(weights)
        else:
            X_weighted = X

        # è®¡ç®—æ¬§æ°è·ç¦»çŸ©é˜µ
        # ä½¿ç”¨å¹¿æ’­è®¡ç®—æ‰€æœ‰ç‚¹å¯¹çš„è·ç¦»
        # ||xi - xj||Â² = ||xi||Â² + ||xj||Â² - 2*xiÂ·xj
        X_norm = np.sum(X_weighted ** 2, axis=1, keepdims=True)
        distances_squared = X_norm + X_norm.T - 2 * np.dot(X_weighted, X_weighted.T)

        # æ•°å€¼ç¨³å®šæ€§ï¼šç¡®ä¿è·ç¦»éè´Ÿ
        distances_squared = np.maximum(distances_squared, 0)

        # è®¡ç®—é«˜æ–¯ç›¸ä¼¼åº¦
        # sigma fixed to 1.0 (Gu et al., 2025)
        Q = np.exp(-distances_squared / (2 * self.sigma ** 2))

        # å¦‚æœæŒ‡å®škè¿‘é‚»ï¼Œåªä¿ç•™kä¸ªæœ€è¿‘é‚»
        if k_neighbors is not None and k_neighbors < n_samples - 1:
            for i in range(n_samples):
                # æ‰¾åˆ°ç¬¬k+1è¿‘çš„è·ç¦»ä½œä¸ºé˜ˆå€¼
                sorted_indices = np.argsort(distances_squared[i])
                # ä¿ç•™è‡ªå·±å’Œkä¸ªæœ€è¿‘é‚»
                threshold_idx = min(k_neighbors + 1, n_samples)
                keep_indices = sorted_indices[:threshold_idx]

                # åˆ›å»ºæ©ç 
                mask = np.zeros(n_samples, dtype=bool)
                mask[keep_indices] = True

                # åº”ç”¨æ©ç 
                Q[i, ~mask] = 0
                Q[~mask, i] = 0

        # å¯¹è§’çº¿è®¾ä¸º0ï¼ˆä¸åŒ…æ‹¬è‡ªç›¸ä¼¼ï¼‰
        np.fill_diagonal(Q, 0)

        # å¯¹ç§°åŒ–ï¼ˆç¡®ä¿æ•°å€¼å¯¹ç§°æ€§ï¼‰
        Q = (Q + Q.T) / 2

        logger.debug(f"ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºå®Œæˆ: å½¢çŠ¶={Q.shape}, éé›¶å…ƒç´ æ¯”ä¾‹={np.count_nonzero(Q) / Q.size:.2%}")

        return Q

    def _build_similarity_matrix_gpu(self,
                                      X: np.ndarray,
                                      weights: Optional[np.ndarray] = None,
                                      k_neighbors: Optional[int] = None) -> np.ndarray:
        """GPUç‰ˆæœ¬ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºï¼Œæ•°å­¦ä¸Šç­‰ä»·äºCPUå®ç°ã€‚"""
        if not self.use_gpu or self._torch is None:
            raise RuntimeError("GPUè·¯å¾„ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒç”¨_build_similarity_matrix_gpu")

        torch_module = self._torch
        device = self._torch_device
        n_samples = X.shape[0]
        batch_size = min(5000, n_samples)
        gaussian_denominator = 2.0 * (self.sigma ** 2)

        Q_batches: List[np.ndarray] = []

        with torch_module.no_grad():
            X_gpu = torch_module.as_tensor(X, dtype=torch_module.double, device=device)

            if weights is not None:
                if weights.ndim == 1:
                    weights_gpu = torch_module.as_tensor(weights, dtype=torch_module.double, device=device)
                    X_weighted = X_gpu * torch_module.sqrt(weights_gpu.view(1, -1))
                else:
                    weights_gpu = torch_module.as_tensor(weights, dtype=torch_module.double, device=device)
                    X_weighted = X_gpu * torch_module.sqrt(weights_gpu)
            else:
                X_weighted = X_gpu

            norm_all = torch_module.sum(X_weighted ** 2, dim=1, keepdim=True).T

            for start_idx in range(0, n_samples, batch_size):
                end_idx = min(start_idx + batch_size, n_samples)
                X_batch = X_weighted[start_idx:end_idx]
                norm_batch = torch_module.sum(X_batch ** 2, dim=1, keepdim=True)

                distances_sq = norm_batch + norm_all - 2.0 * torch_module.mm(X_batch, X_weighted.T)
                distances_sq = torch_module.clamp(distances_sq, min=0.0)

                Q_batch = torch_module.exp(-distances_sq / gaussian_denominator)

                if k_neighbors is not None and k_neighbors < n_samples - 1:
                    threshold = min(k_neighbors + 1, n_samples)
                    for local_idx in range(end_idx - start_idx):
                        sorted_indices = torch_module.argsort(distances_sq[local_idx])
                        keep_indices = sorted_indices[:threshold]
                        mask = torch_module.zeros(n_samples, dtype=torch_module.bool, device=device)
                        mask[keep_indices] = True
                        Q_batch[local_idx, ~mask] = 0.0

                Q_batches.append(Q_batch.cpu().numpy())

                if self.use_gpu:
                    del X_batch, norm_batch, distances_sq, Q_batch
                    torch_module.cuda.empty_cache()

        Q = np.vstack(Q_batches)
        np.fill_diagonal(Q, 0.0)
        Q = (Q + Q.T) / 2.0

        if self.use_gpu:
            del X_gpu, X_weighted, norm_all
            if 'weights_gpu' in locals():
                del weights_gpu
            if torch_module.cuda.is_available():
                torch_module.cuda.empty_cache()

        return Q

    def construct_laplacian(self, Q: np.ndarray) -> np.ndarray:
        """
        æ„å»ºå›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ

        L = D - Q
        å…¶ä¸­Dä¸ºåº¦çŸ©é˜µï¼ŒDii = Î£Qij

        Args:
            Q: ç›¸ä¼¼åº¦çŸ©é˜µ

        Returns:
            æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µL
        """
        if self.use_gpu and self._torch is not None:
            return self._construct_laplacian_gpu(Q)

        # è®¡ç®—åº¦çŸ©é˜µ
        degrees = np.sum(Q, axis=1)
        D = np.diag(degrees)

        # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        L = D - Q

        # å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯ï¼ˆå¯é€‰ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
        # L_norm = D^(-1/2) * L * D^(-1/2)
        # é¿å…é™¤é›¶
        degrees_sqrt_inv = np.zeros_like(degrees)
        non_zero_mask = degrees > 1e-10
        degrees_sqrt_inv[non_zero_mask] = 1.0 / np.sqrt(degrees[non_zero_mask])

        D_sqrt_inv = np.diag(degrees_sqrt_inv)
        L_normalized = D_sqrt_inv @ L @ D_sqrt_inv

        # ç¡®ä¿å¯¹ç§°æ€§
        L_normalized = (L_normalized + L_normalized.T) / 2

        return L_normalized

    def _construct_laplacian_gpu(self, Q: np.ndarray) -> np.ndarray:
        """GPUç‰ˆæœ¬æ‹‰æ™®æ‹‰æ–¯æ„å»ºï¼Œä½¿ç”¨å¹¿æ’­ä»£æ›¿æ˜¾å¼å¯¹è§’çŸ©é˜µã€‚"""
        if not self.use_gpu or self._torch is None:
            raise RuntimeError("GPUè·¯å¾„ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒç”¨_construct_laplacian_gpu")

        torch_module = self._torch
        device = self._torch_device
        n_samples = Q.shape[0]
        chunk_size = min(5000, n_samples)

        with torch_module.no_grad():
            degrees = torch_module.from_numpy(Q.sum(axis=1)).to(device=device, dtype=torch_module.double)
            degrees_sqrt_inv = torch_module.zeros_like(degrees)
            non_zero_mask = degrees > 1e-10
            degrees_sqrt_inv[non_zero_mask] = torch_module.pow(degrees[non_zero_mask], -0.5)
            col_scale = degrees_sqrt_inv.unsqueeze(0)

            L_normalized_chunks: List[np.ndarray] = []

            for start_idx in range(0, n_samples, chunk_size):
                end_idx = min(start_idx + chunk_size, n_samples)
                Q_chunk = torch_module.from_numpy(Q[start_idx:end_idx]).to(device=device, dtype=torch_module.double)

                L_chunk = -Q_chunk
                row_indices = torch_module.arange(end_idx - start_idx, device=device)
                col_indices = torch_module.arange(start_idx, end_idx, device=device)
                L_chunk[row_indices, col_indices] += degrees[start_idx:end_idx]

                row_scale = degrees_sqrt_inv[start_idx:end_idx].unsqueeze(1)
                L_normalized_chunk = row_scale * L_chunk * col_scale

                L_normalized_chunks.append(L_normalized_chunk.cpu().numpy())

                del Q_chunk, L_chunk, L_normalized_chunk, row_scale, row_indices, col_indices
                if torch_module.cuda.is_available():
                    torch_module.cuda.empty_cache()

            L_normalized = np.vstack(L_normalized_chunks)
            L_normalized = (L_normalized + L_normalized.T) / 2.0

            del degrees, degrees_sqrt_inv, col_scale, L_normalized_chunks
            if torch_module.cuda.is_available():
                torch_module.cuda.empty_cache()

        return L_normalized

    def _admm_optimization_gpu(self,
                               X: np.ndarray,
                               L: np.ndarray) -> np.ndarray:
        """
        GPUåŠ é€Ÿç‰ˆ ADMM è®¡ç®—ï¼Œé‡‡ç”¨åˆ†å—ä¸ç¨€ç–çº¿æ€§ç®—å­é¿å…æ˜¾å­˜æš´æ¶¨ã€‚
        """
        if not self.use_gpu or self._torch is None:
            raise RuntimeError("GPUè·¯å¾„ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒç”¨_admm_optimization_gpu")

        torch_module = self._torch
        device = self._torch_device

        n_samples, n_features = X.shape
        d = self.target_dim

        alpha = self.alpha
        beta = self.beta
        tol = self.tol
        max_iter = self.max_iter

        sample_chunk = min(5000, n_samples)

        # ========== æ–°å¢ï¼šæ—©åœæ£€æµ‹å‡½æ•° ==========
        def check_early_stopping(iter_idx: int,
                                 relative_change: float,
                                 obj_history: List[float],
                                 rel_history: List[float]) -> Tuple[bool, str]:
            """æ™ºèƒ½æ—©åœæ£€æµ‹"""
            if relative_change < tol:
                return True, "Fæ”¶æ•›"

            if iter_idx >= 5 and len(obj_history) >= 5:
                recent = obj_history[-5:]
                improvement = (recent[0] - recent[-1]) / (abs(recent[0]) + 1e-10)
                if improvement < 1e-4:
                    return True, "ç›®æ ‡å‡½æ•°åœæ»"

            if iter_idx >= 3 and len(rel_history) >= 3:
                if all(history_value < tol * 10 for history_value in rel_history[-3:]):
                    return True, "FæŒ¯è¡æ”¶æ•›"

            if iter_idx >= 20 and relative_change < tol * 50:
                return True, "å¿«é€Ÿæ”¶æ•›"

            return False, ""

        # ========== æ–°å¢ï¼šè¿›åº¦æ¡æ›´æ–°å‡½æ•° ==========
        def update_progress(iter_idx: int,
                            max_iterations: int,
                            objective: float,
                            rel_change: float,
                            phase: str = "è®¡ç®—ä¸­") -> None:
            """å•è¡Œè¿›åº¦æ¡æ›´æ–°"""
            if max_iterations <= 0:
                progress = 100.0
                filled = 30
            else:
                progress = (iter_idx + 1) / max_iterations * 100
                filled = min(30, int(30 * (iter_idx + 1) / max_iterations))

            bar = "â–ˆ" * filled + "â–‘" * (30 - filled)
            sys.stdout.write(
                f'\r  ADMMä¼˜åŒ– [{bar}] {progress:.1f}% | '
                f'è¿­ä»£:{iter_idx + 1}/{max_iterations} | '
                f'ç›®æ ‡å€¼:{objective:.4f} | '
                f'ç›¸å¯¹å˜åŒ–:{rel_change:.2e} | '
                f'{phase}'
            )
            sys.stdout.flush()

        logger.info("å¼€å§‹ADMMè¿­ä»£ï¼ˆæ—©åœ+è¿›åº¦æ¡ï¼‰...")
        start_time = time.time()

        X_gpu = torch_module.as_tensor(X, dtype=torch_module.double, device=device)

        A = torch_module.zeros((n_features, d), dtype=torch_module.double, device=device)
        F = torch_module.randn((n_samples, d), dtype=torch_module.double, device=device)
        q, _ = torch_module.linalg.qr(F, mode='reduced')
        F = q[:, :d]

        XTX = torch_module.zeros((n_features, n_features), dtype=torch_module.double, device=device)
        for start_idx in range(0, n_samples, sample_chunk):
            end_idx = min(start_idx + sample_chunk, n_samples)
            X_chunk = X_gpu[start_idx:end_idx]
            XTX = XTX + X_chunk.T @ X_chunk
        XTX_diag = torch_module.diag(XTX) + 1e-10

        optimization_history = {
            'objective': [],
            'constraint_violation': [],
            'relative_change': [],
            'iterations': 0
        }

        for iter_idx in range(max_iter):
            F_old = F.clone()

            if self.use_sparse_matrix:
                phase_label = "ç‰¹å¾åˆ†è§£(CPU-ç¨€ç–)"
            elif self.use_float32_eigh and torch_module.cuda.is_available():
                phase_label = "ç‰¹å¾åˆ†è§£(GPU-lobpcg)"
            else:
                phase_label = "ç‰¹å¾åˆ†è§£(CPU)"

            update_progress(
                iter_idx,
                max_iter,
                optimization_history['objective'][-1] if optimization_history['objective'] else 0.0,
                optimization_history['relative_change'][-1] if optimization_history['relative_change'] else 0.0,
                phase_label
            )

            if self.use_sparse_matrix:
                try:
                    if iter_idx == 0:
                        logger.info(f"ä½¿ç”¨ç¨€ç–ç‰¹å¾åˆ†è§£ï¼ˆCPU eigshï¼‰è®¡ç®— {d} ä¸ªæœ€å°ç‰¹å¾å€¼...")

                    from scipy.sparse import csr_matrix
                    import scipy.sparse

                    L_sparse = csr_matrix(L)
                    M_sparse = L_sparse + alpha * scipy.sparse.eye(n_samples, format="csr")

                    eigenvalues, eigenvectors = eigsh(
                        M_sparse,
                        k=d,
                        which="SM",
                        maxiter=1000,
                        tol=1e-6,
                    )

                    F = torch_module.from_numpy(eigenvectors).to(device=device, dtype=torch_module.float64)
                    if iter_idx == 0:
                        print("âœ“ ç¨€ç–ç‰¹å¾åˆ†è§£ï¼ˆCPU eigshï¼‰å®Œæˆï¼Œç»“æœå·²ä¼ è¾“åˆ° GPU", flush=True)

                    del L_sparse, M_sparse, eigenvalues, eigenvectors

                except Exception as e:
                    if iter_idx == 0:
                        logger.warning(f"ç¨€ç–ç‰¹å¾åˆ†è§£å¤±è´¥: {type(e).__name__}: {e}")
                        logger.info("å›é€€åˆ° CPU å¯†é›†ç‰¹å¾åˆ†è§£...")

                    M_cpu = L + alpha * np.eye(n_samples, dtype=np.float64)
                    eigenvalues, eigenvectors = eigsh(M_cpu, k=d, which='SM')
                    F = torch_module.from_numpy(eigenvectors).to(device=device, dtype=torch_module.double)
                    del M_cpu, eigenvalues, eigenvectors
                    if iter_idx == 0:
                        logger.info("CPU eigsh æ‰§è¡ŒæˆåŠŸ")

            elif self.use_float32_eigh and torch_module.cuda.is_available():
                try:
                    logger.info(f"å°è¯•ä½¿ç”¨ GPU lobpcg è®¡ç®— {d} ä¸ªæœ€å°ç‰¹å¾å€¼...")

                    M_gpu = torch_module.from_numpy(L).to(device=device, dtype=torch_module.float64)
                    M_gpu = M_gpu + alpha * torch_module.eye(n_samples, dtype=torch_module.float64, device=device)

                    if torch_module.isnan(M_gpu).any() or torch_module.isinf(M_gpu).any():
                        raise ValueError("M_gpu åŒ…å« NaN æˆ– Infï¼Œæ— æ³•è¿›è¡Œç‰¹å¾åˆ†è§£")

                    eigenvalues, eigenvectors = torch_module.lobpcg(
                        M_gpu,
                        k=d,
                        largest=False,
                        niter=300,
                        tol=1e-6,
                        method='ortho'
                    )

                    if eigenvectors.shape[1] < d:
                        raise RuntimeError(f"lobpcg åªæ”¶æ•›äº† {eigenvectors.shape[1]}/{d} ä¸ªç‰¹å¾å‘é‡")

                    orth_check = eigenvectors.T @ eigenvectors
                    orth_error = torch_module.norm(
                        orth_check - torch_module.eye(d, dtype=torch_module.float64, device=device)
                    ).item()

                    if orth_error > 1e-3:
                        logger.warning(f"GPU lobpcg æ­£äº¤æ€§è¯¯å·®: {orth_error:.2e}")
                    else:
                        logger.debug(f"GPU lobpcg æ­£äº¤æ€§éªŒè¯é€šè¿‡: {orth_error:.2e}")

                    F = eigenvectors
                    logger.info(f"GPU lobpcg æˆåŠŸï¼Œæ­£äº¤æ€§è¯¯å·®: {orth_error:.2e}")

                    del M_gpu, eigenvalues
                    if torch_module.cuda.is_available():
                        torch_module.cuda.empty_cache()

                except Exception as e:
                    logger.warning(f"GPU lobpcg å¤±è´¥: {type(e).__name__}: {e}")
                    logger.info("å›é€€åˆ° CPU eigsh æ–¹æ³•...")

                    if 'M_gpu' in locals():
                        del M_gpu
                    if torch_module.cuda.is_available():
                        torch_module.cuda.empty_cache()

                    M_cpu = L + alpha * np.eye(n_samples, dtype=np.float64)
                    eigenvalues, eigenvectors = eigsh(M_cpu, k=d, which='SM')
                    F = torch_module.from_numpy(eigenvectors).to(device=device, dtype=torch_module.double)
                    del M_cpu, eigenvalues, eigenvectors
                    logger.info("CPU eigsh æ‰§è¡ŒæˆåŠŸ")

            else:
                M_cpu = L + alpha * np.eye(n_samples, dtype=np.float64)
                eigenvalues, eigenvectors = eigsh(M_cpu, k=d, which='SM')
                F = torch_module.from_numpy(eigenvectors).to(device=device, dtype=torch_module.double)
                del M_cpu, eigenvalues, eigenvectors

            if torch_module.count_nonzero(A).item() > 0:
                XA = torch_module.zeros((n_samples, d), dtype=torch_module.double, device=device)
                for start_idx in range(0, n_samples, sample_chunk):
                    end_idx = min(start_idx + sample_chunk, n_samples)
                    XA[start_idx:end_idx] = X_gpu[start_idx:end_idx] @ A
                correction = (self.alpha * XA) / (1.0 + self.alpha)
                F = F + correction
                q, _ = torch_module.linalg.qr(F, mode='reduced')
                F = q[:, :d]
                del XA, correction, q
                if torch_module.cuda.is_available():
                    torch_module.cuda.empty_cache()

            update_progress(
                iter_idx,
                max_iter,
                optimization_history['objective'][-1] if optimization_history['objective'] else 0.0,
                optimization_history['relative_change'][-1] if optimization_history['relative_change'] else 0.0,
                "æ›´æ–°AçŸ©é˜µ"
            )

            XTF = torch_module.zeros((n_features, d), dtype=torch_module.double, device=device)
            for start_idx in range(0, n_samples, sample_chunk):
                end_idx = min(start_idx + sample_chunk, n_samples)
                X_chunk = X_gpu[start_idx:end_idx]
                F_chunk = F[start_idx:end_idx]
                XTF = XTF + X_chunk.T @ F_chunk

            for j in range(d):
                residual = XTF[:, j] if j == 0 else XTF[:, j] - XTX @ A[:, j]

                norm_residual = torch_module.linalg.norm(residual)
                threshold = beta / (2 * alpha)
                if norm_residual > threshold:
                    shrink = 1 - threshold / (norm_residual + 1e-12)
                    A[:, j] = shrink * residual / XTX_diag
                else:
                    A[:, j].zero_()

            update_progress(
                iter_idx,
                max_iter,
                optimization_history['objective'][-1] if optimization_history['objective'] else 0.0,
                optimization_history['relative_change'][-1] if optimization_history['relative_change'] else 0.0,
                "æ”¶æ•›åˆ¤æ–­"
            )

            F_change = torch_module.linalg.norm(F - F_old, ord='fro').item()
            F_reference = torch_module.linalg.norm(F_old, ord='fro').item() + 1e-10
            relative_change = F_change / F_reference

            trace_term = 0.0
            laplacian_chunk = min(2048, n_samples)
            for row_start in range(0, n_samples, laplacian_chunk):
                row_end = min(row_start + laplacian_chunk, n_samples)
                L_chunk = torch_module.from_numpy(L[row_start:row_end]).to(device=device, dtype=torch_module.double)
                LF_chunk = L_chunk @ F
                trace_term += torch_module.trace(F[row_start:row_end].T @ LF_chunk).item()
                del L_chunk, LF_chunk
                if torch_module.cuda.is_available():
                    torch_module.cuda.empty_cache()

            XA_full = torch_module.zeros((n_samples, d), dtype=torch_module.double, device=device)
            for start_idx in range(0, n_samples, sample_chunk):
                end_idx = min(start_idx + sample_chunk, n_samples)
                XA_full[start_idx:end_idx] = X_gpu[start_idx:end_idx] @ A

            reconstruction_term = alpha * (torch_module.linalg.norm(XA_full - F, ord='fro') ** 2).item()
            regularizer = beta * torch_module.sum(torch_module.linalg.norm(A, dim=0)).item()
            objective = trace_term + reconstruction_term + regularizer

            optimization_history['objective'].append(objective)
            optimization_history['constraint_violation'].append(F_change)
            optimization_history['relative_change'].append(relative_change)
            optimization_history['iterations'] = iter_idx + 1

            update_progress(iter_idx, max_iter, objective, relative_change, "æ£€æŸ¥æ”¶æ•›")

            should_stop, reason = check_early_stopping(
                iter_idx,
                relative_change,
                optimization_history['objective'],
                optimization_history['relative_change']
            )

            if should_stop:
                elapsed = time.time() - start_time
                sys.stdout.write('\n')
                logger.info(f"  âœ… ADMMæå‰æ”¶æ•›ï¼åŸå› : {reason}")
                logger.info(f"     å®é™…è¿­ä»£: {iter_idx + 1}/{max_iter} | è€—æ—¶: {elapsed / 60:.1f}åˆ†é’Ÿ")
                del XA_full, XTF
                if torch_module.cuda.is_available():
                    torch_module.cuda.empty_cache()
                break

            del XA_full, XTF

            if torch_module.cuda.is_available() and (iter_idx + 1) % 10 == 0:
                torch_module.cuda.empty_cache()
        else:
            elapsed = time.time() - start_time
            sys.stdout.write('\n')
            logger.info(f"  âš ï¸ ADMMè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{max_iter}")
            logger.info(f"     æ€»è€—æ—¶: {elapsed / 60:.1f}åˆ†é’Ÿ")

        A_cpu = A.detach().cpu().numpy()
        del X_gpu, A, F, XTX
        if torch_module.cuda.is_available():
            torch_module.cuda.empty_cache()

        self.optimization_history = optimization_history
        return A_cpu

    def admm_optimization(self,
                         X: np.ndarray,
                         L: np.ndarray) -> np.ndarray:
        """
        ADMMç®—æ³•æ±‚è§£æ˜ å°„çŸ©é˜µ

        ä¼˜åŒ–ç›®æ ‡ï¼š
        min tr(F^T*L*F) + Î±||ZA-F||Â²_F + Î²||A||_2,1
        s.t. F^T*F = I

        è¿­ä»£æ­¥éª¤ï¼š
        1. Få­é—®é¢˜ï¼šå›ºå®šAï¼Œæ›´æ–°F
        2. Aå­é—®é¢˜ï¼šå›ºå®šFï¼Œæ›´æ–°A
        3. æ”¶æ•›åˆ¤æ–­ï¼š||F(k+1)-F(k)||_F < Îµ

        Args:
            X: è¾“å…¥æ•°æ®çŸ©é˜µ Z (n_samples x n_features)
            L: æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ (n_samples x n_samples)

        Returns:
            æ˜ å°„çŸ©é˜µA (n_features x target_dim)
        """
        n_samples, n_features = X.shape
        d = self.target_dim

        self.optimization_history = {
            'objective': [],
            'constraint_violation': [],
            'relative_change': [],
            'iterations': 0
        }

        # åˆå§‹åŒ–
        A = np.zeros((n_features, d))  # æ˜ å°„çŸ©é˜µ
        F = np.random.randn(n_samples, d)  # ä½ç»´è¡¨ç¤º
        F = F / np.linalg.norm(F, axis=0, keepdims=True)  # åˆ—å½’ä¸€åŒ–

        # ADMMè¿­ä»£
        for iter_idx in range(self.max_iter):
            F_old = F.copy()

            # ========== Step 1: æ›´æ–°Fï¼ˆå›ºå®šAï¼‰ ==========
            # ä¼˜åŒ–ç›®æ ‡ï¼šmin tr(F^T*L*F) + Î±||X*A - F||Â²_F
            # ç­‰ä»·äºï¼šmin tr(F^T*(L + Î±I)*F) - 2Î±*tr(F^T*X*A)

            # æ„é€ çŸ©é˜µM = L + Î±I
            M = L + self.alpha * np.eye(n_samples)

            # ç›®æ ‡ï¼šF^T*M*F - 2Î±*F^T*X*A
            # ä½¿ç”¨ç‰¹å¾åˆ†è§£æ±‚è§£
            if n_features < 500:  # å°è§„æ¨¡é—®é¢˜ï¼Œç›´æ¥æ±‚è§£
                # è®¡ç®—Mçš„ç‰¹å¾åˆ†è§£
                eigenvalues, eigenvectors = eigh(M)

                # é€‰æ‹©æœ€å°çš„dä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
                idx = np.argsort(eigenvalues)[:d]
                F = eigenvectors[:, idx]
            else:  # å¤§è§„æ¨¡é—®é¢˜ï¼Œä½¿ç”¨ç¨€ç–æ±‚è§£
                # ä½¿ç”¨Lanczosæ–¹æ³•æ±‚è§£æœ€å°ç‰¹å¾å€¼
                eigenvalues, eigenvectors = eigsh(M, k=d, which='SM')
                F = eigenvectors

            # å¦‚æœAä¸ä¸ºé›¶ï¼Œéœ€è¦è€ƒè™‘X*Aé¡¹
            if np.any(A != 0):
                XA = X @ A
                # ä¿®æ­£Fä½¿å…¶æ›´æ¥è¿‘XA
                F = F + self.alpha * XA / (1 + self.alpha)
                # é‡æ–°æ­£äº¤åŒ–
                U, S, Vt = np.linalg.svd(F, full_matrices=False)
                F = U @ Vt

            # ========== Step 2: æ›´æ–°Aï¼ˆå›ºå®šFï¼‰ ==========
            # ä¼˜åŒ–ç›®æ ‡ï¼šmin Î±||X*A - F||Â²_F + Î²||A||_2,1
            # è¿™æ˜¯ä¸€ä¸ªL2,1æ­£åˆ™åŒ–çš„æœ€å°äºŒä¹˜é—®é¢˜

            # è®¡ç®—X^T*X
            XTX = X.T @ X
            XTF = X.T @ F

            # ä½¿ç”¨è½¯é˜ˆå€¼ç®—å­æ±‚è§£
            for j in range(d):
                # å¯¹æ¯ä¸€åˆ—å•ç‹¬æ±‚è§£
                # æ¢¯åº¦ï¼š2Î±*X^T*(X*aj - fj)
                # L2,1æ­£åˆ™åŒ–çš„è¿‘ç«¯ç®—å­

                # è®¡ç®—æ®‹å·®
                if j == 0:
                    residual = XTF[:, j]
                else:
                    residual = XTF[:, j] - XTX @ A[:, j]

                # è½¯é˜ˆå€¼
                norm_residual = np.linalg.norm(residual)
                if norm_residual > self.beta / (2 * self.alpha):
                    A[:, j] = (1 - self.beta / (2 * self.alpha * norm_residual)) * \
                             residual / (np.diagonal(XTX) + 1e-10)
                else:
                    A[:, j] = 0

            # ========== Step 3: æ”¶æ•›åˆ¤æ–­ ==========
            F_change = np.linalg.norm(F - F_old, 'fro')
            relative_change = F_change / (np.linalg.norm(F_old, 'fro') + 1e-10)

            # è®¡ç®—ç›®æ ‡å‡½æ•°å€¼
            obj_value = np.trace(F.T @ L @ F) + \
                       self.alpha * np.linalg.norm(X @ A - F, 'fro') ** 2 + \
                       self.beta * np.sum(np.linalg.norm(A, axis=0))

            self.optimization_history['objective'].append(obj_value)
            self.optimization_history['constraint_violation'].append(F_change)
            self.optimization_history['relative_change'].append(relative_change)
            self.optimization_history['iterations'] = iter_idx + 1

            # æ—¥å¿—è¾“å‡ºï¼ˆæ¯10æ¬¡è¿­ä»£ï¼‰
            if (iter_idx + 1) % 10 == 0:
                logger.debug(f"ADMMè¿­ä»£{iter_idx + 1}: ç›®æ ‡å€¼={obj_value:.4f}, "
                           f"ç›¸å¯¹å˜åŒ–={relative_change:.6f}")

            # æ”¶æ•›åˆ¤æ–­
            if relative_change < self.tol:
                logger.info(f"ADMMæ”¶æ•›äºç¬¬{iter_idx + 1}æ¬¡è¿­ä»£")
                break

        else:
            logger.warning(f"ADMMè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{self.max_iter}ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›")
            self.optimization_history['iterations'] = self.max_iter

        return A

    def fit(self,
            X_train: Union[np.ndarray, pd.DataFrame],
            dpsr_weights: Optional[Union[np.ndarray, Dict]] = None) -> 'DLFE':
        """
        ä»è®­ç»ƒé›†å­¦ä¹ æ˜ å°„çŸ©é˜µA

        1. æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        2. è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        3. ADMMä¼˜åŒ–æ±‚è§£A
        4. å­˜å‚¨æ˜ å°„çŸ©é˜µä¾›transformä½¿ç”¨

        Args:
            X_train: è®­ç»ƒæ•°æ® (n_samples x n_features)
            dpsr_weights: DPSRæä¾›çš„åŠ¨æ€æƒé‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            self: è¿”å›è‡ªèº«ä»¥æ”¯æŒé“¾å¼è°ƒç”¨
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(X_train, pd.DataFrame):
            X = X_train.values
        else:
            X = np.asarray(X_train)

        n_samples, n_features = X.shape
        logger.info(f"å¼€å§‹DLFEè®­ç»ƒ: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾ -> {self.target_dim}ç»´")

        # å¤„ç†DPSRæƒé‡
        if dpsr_weights is not None:
            if isinstance(dpsr_weights, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œè®¡ç®—å¹³å‡æƒé‡
                weights_list = list(dpsr_weights.values())
                avg_weights = np.mean(weights_list, axis=0)
            else:
                avg_weights = dpsr_weights
        else:
            avg_weights = None

        # Step 1: æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        logger.info("æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ...")
        Q = self.build_similarity_matrix(X, weights=avg_weights, k_neighbors=min(50, n_samples - 1))

        # Step 2: æ„å»ºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        logger.info("æ„å»ºæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ...")
        L = self.construct_laplacian(Q)

        # ========== è¯Šæ–­çŸ©é˜µç¨€ç–æ€§ ==========
        logger.info("\nå¼€å§‹è¯Šæ–­çŸ©é˜µç¨€ç–æ€§...")
        Q_diagnosis = diagnose_matrix_sparsity(Q, "ç›¸ä¼¼åº¦çŸ©é˜µ Q")
        L_diagnosis = diagnose_matrix_sparsity(L, "æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ L")
        self._sparsity_diagnosis = {
            "Q": Q_diagnosis,
            "L": L_diagnosis,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        if L_diagnosis["should_use_sparse"]:
            logger.warning("âš ï¸  æ£€æµ‹åˆ°æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µLé«˜åº¦ç¨€ç–ï¼")
            logger.warning("    å½“å‰å°†ä½¿ç”¨å¯†é›†çŸ©é˜µï¼ˆéœ€è¦ %.2f GB GPUæ˜¾å­˜ï¼‰", L_diagnosis["dense_memory_gb"])
            logger.warning("    å»ºè®®å¯ç”¨ç¨€ç–çŸ©é˜µä¼˜åŒ–ï¼ˆä»…éœ€ %.2f MBï¼‰", L_diagnosis["sparse_memory_mb"])
            logger.warning("    å¯èŠ‚çœ %.1fx å†…å­˜ï¼", L_diagnosis["savings_ratio"])

        # Clear GPU cache after matrix construction to avoid OOM in ADMM.
        if self.use_gpu and self._torch is not None:
            import gc

            gc.collect()
            if self._torch.cuda.is_available():
                self._torch.cuda.empty_cache()
                allocated = self._torch.cuda.memory_allocated() / 1024 ** 3
                logger.info("âœ“ çŸ©é˜µæ„å»ºåæ¸…ç†GPU - å·²åˆ†é…: %.2f GB", allocated)

        # Step 3: ADMMä¼˜åŒ–
        logger.info("å¼€å§‹ADMMä¼˜åŒ–...")
        if self.use_gpu and self._torch is not None:
            logger.info("DLFEä½¿ç”¨GPUè·¯å¾„æ‰§è¡ŒADMMä¼˜åŒ–")
            self.mapping_matrix = self._admm_optimization_gpu(X, L)
        else:
            logger.info("DLFEä½¿ç”¨CPUè·¯å¾„æ‰§è¡ŒADMMä¼˜åŒ–")
            self.mapping_matrix = self.admm_optimization(X, L)

        self.is_fitted = True

        logger.info(f"DLFEè®­ç»ƒå®Œæˆï¼Œæ˜ å°„çŸ©é˜µå½¢çŠ¶: {self.mapping_matrix.shape}")

        return self

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        åº”ç”¨å­¦ä¹ åˆ°çš„æ˜ å°„çŸ©é˜µ

        F = X * A
        å°†è¾“å…¥é™ç»´åˆ°30ç»´

        Args:
            X: è¾“å…¥æ•°æ® (n_samples x n_features)

        Returns:
            é™ç»´åçš„ç‰¹å¾ (n_samples x target_dim)
        """
        if not self.is_fitted:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fitæ–¹æ³•å­¦ä¹ æ˜ å°„çŸ©é˜µ")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = np.asarray(X)

        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        if X_array.shape[1] != self.mapping_matrix.shape[0]:
            raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: è¾“å…¥{X_array.shape[1]}ç»´, "
                           f"æœŸæœ›{self.mapping_matrix.shape[0]}ç»´")

        # åº”ç”¨æ˜ å°„
        F = X_array @ self.mapping_matrix

        # å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
        # F = F / (np.linalg.norm(F, axis=1, keepdims=True) + 1e-10)

        return F

    def fit_transform(self,
                     X_train: Union[np.ndarray, pd.DataFrame],
                     dpsr_weights: Optional[Union[np.ndarray, Dict]] = None) -> np.ndarray:
        """
        ç»„åˆfitå’Œtransform

        Args:
            X_train: è®­ç»ƒæ•°æ®
            dpsr_weights: DPSRæƒé‡

        Returns:
            é™ç»´åçš„ç‰¹å¾ (n_samples x target_dim)
        """
        self.fit(X_train, dpsr_weights)
        return self.transform(X_train)

    def save_mapping(self, filepath: Union[str, Path]) -> None:
        """
        ä¿å­˜æ˜ å°„çŸ©é˜µ

        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ˜ å°„çŸ©é˜µå¯ä¿å­˜")

        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        mapping_data = {
            'mapping_matrix': self.mapping_matrix.tolist(),
            'target_dim': self.target_dim,
            'sigma': self.sigma,
            'alpha': self.alpha,
            'beta': self.beta,
            'optimization_history': self.optimization_history
        }

        if filepath.suffix == '.json':
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(mapping_data, f, indent=2)
        elif filepath.suffix == '.pkl':
            # pickleå¯ä»¥ç›´æ¥ä¿å­˜numpyæ•°ç»„
            mapping_data['mapping_matrix'] = self.mapping_matrix
            with open(filepath, 'wb') as f:
                pickle.dump(mapping_data, f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath.suffix}")

        logger.info(f"DLFEæ˜ å°„çŸ©é˜µå·²ä¿å­˜: {filepath}")

    def load_mapping(self, filepath: Union[str, Path]) -> None:
        """
        åŠ è½½æ˜ å°„çŸ©é˜µ

        Args:
            filepath: æ˜ å°„æ–‡ä»¶è·¯å¾„
        """
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

        if filepath.suffix == '.json':
            with open(filepath, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            self.mapping_matrix = np.array(mapping_data['mapping_matrix'])
        elif filepath.suffix == '.pkl':
            with open(filepath, 'rb') as f:
                mapping_data = pickle.load(f)
            self.mapping_matrix = mapping_data['mapping_matrix']
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath.suffix}")

        self.target_dim = mapping_data['target_dim']
        self.sigma = mapping_data['sigma']
        self.alpha = mapping_data['alpha']
        self.beta = mapping_data['beta']
        self.optimization_history = mapping_data.get('optimization_history', {})
        self.is_fitted = True

        logger.info(f"DLFEæ˜ å°„çŸ©é˜µå·²åŠ è½½: {filepath}")

    def reconstruction_error(self,
                            X_original: np.ndarray,
                            X_reduced: np.ndarray) -> float:
        """
        è®¡ç®—é‡æ„è¯¯å·®

        Args:
            X_original: åŸå§‹é«˜ç»´æ•°æ®
            X_reduced: é™ç»´åçš„æ•°æ®

        Returns:
            é‡æ„è¯¯å·®
        """
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")

        # å°è¯•é‡æ„ï¼ˆä½¿ç”¨ä¼ªé€†ï¼‰
        A_pinv = np.linalg.pinv(self.mapping_matrix)
        X_reconstructed = X_reduced @ A_pinv.T

        # è®¡ç®—è¯¯å·®
        error = np.mean((X_original - X_reconstructed) ** 2)

        return error


def test_dlfe():
    """æµ‹è¯•DLFEæ¨¡å—"""
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    n_samples = 150
    n_features = 50  # é«˜ç»´è¾“å…¥

    np.random.seed(42)

    # ç”Ÿæˆå…·æœ‰æµå½¢ç»“æ„çš„æ•°æ®
    # ä½¿ç”¨ç‘å£«å·æ•°æ®é›†çš„æ€æƒ³
    t = np.linspace(0, 4 * np.pi, n_samples)
    # 3Dæµå½¢
    X_3d = np.column_stack([
        t * np.cos(t),
        t * np.sin(t),
        t
    ])

    # æ‰©å±•åˆ°é«˜ç»´ï¼ˆæ·»åŠ å™ªå£°ç»´åº¦ï¼‰
    X_high = np.hstack([
        X_3d,
        0.1 * np.random.randn(n_samples, n_features - 3)
    ])

    # åˆ›å»ºæ¨¡æ‹Ÿçš„DPSRæƒé‡
    dpsr_weights = np.random.rand(n_features)
    dpsr_weights = dpsr_weights / np.sum(dpsr_weights)

    # åˆå§‹åŒ–DLFE
    dlfe = DLFE(target_dim=30)

    # è®­ç»ƒ
    print("å¼€å§‹DLFEè®­ç»ƒ...")
    dlfe.fit(X_high, dpsr_weights)

    # è½¬æ¢
    X_embedded = dlfe.transform(X_high)

    print(f"è¾“å…¥å½¢çŠ¶: {X_high.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {X_embedded.shape}")
    print(f"æ˜ å°„çŸ©é˜µå½¢çŠ¶: {dlfe.mapping_matrix.shape}")
    print(f"ä¼˜åŒ–è¿­ä»£æ¬¡æ•°: {dlfe.optimization_history['iterations']}")

    # è®¡ç®—é‡æ„è¯¯å·®
    error = dlfe.reconstruction_error(X_high, X_embedded)
    print(f"é‡æ„è¯¯å·®: {error:.6f}")

    return dlfe, X_embedded


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_dlfe()
